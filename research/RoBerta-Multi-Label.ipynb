{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import contractions\n",
    "import langid\n",
    "import spacy\n",
    "import re\n",
    "import emoji\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn import metrics\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sudeepmungara/Documents/Personal_Projects/NLP/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "splits = {'train': 'simplified/train-00000-of-00001.parquet', 'validation': 'simplified/validation-00000-of-00001.parquet', 'test': 'simplified/test-00000-of-00001.parquet'}\n",
    "train_df = pd.read_parquet(\"hf://datasets/google-research-datasets/go_emotions/\" + splits[\"train\"])\n",
    "valid_df = pd.read_parquet(\"hf://datasets/google-research-datasets/go_emotions/\" + splits[\"validation\"])\n",
    "test_df = pd.read_parquet(\"hf://datasets/google-research-datasets/go_emotions/\" + splits[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = ['admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion', 'curiosity', 'desire', 'disappointment', 'disapproval', 'disgust', 'embarrassment', 'excitement', 'fear', 'gratitude', 'grief', 'joy', 'love', 'nervousness', 'optimism', 'pride', 'realization', 'relief', 'remorse', 'sadness', 'surprise', 'neutral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {i:emotions[i] for i in range(len(emotions))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in id2label:\n",
    "    train_df[id2label[i]] = train_df.labels.apply(lambda x: 1 if i in x else 0)\n",
    "    valid_df[id2label[i]] = valid_df.labels.apply(lambda x: 1 if i in x else 0)\n",
    "    test_df[id2label[i]] = test_df.labels.apply(lambda x: 1 if i in x else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demojize_text(text):\n",
    "    return emoji.demojize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "\n",
    "    # Ensure the input is a string\n",
    "    text = str(text).lower()\n",
    "    \n",
    "    # Remove specific patterns and unwanted characters\n",
    "    text = re.sub(r'\\:(.*?)\\:', '', text)\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)\n",
    "    \n",
    "    # Remove HTML content\n",
    "    text = BeautifulSoup(text, 'html.parser').get_text()\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    \n",
    "    # Remove HTML tags, newlines, and words with numbers\n",
    "    text = re.sub(r'<.*?>+', '', text)\n",
    "    text = re.sub(r'\\n', '', text)\n",
    "    text = re.sub(r'\\w*\\d\\w*', '', text)\n",
    "    \n",
    "    # Remove all punctuation\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)  # Removes everything except word characters and spaces\n",
    "    \n",
    "    # Replace multiple spaces with a single space\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    new_text=[]\n",
    "    for word in text.split():\n",
    "        if word in stopwords.words('english'):\n",
    "            new_text.append('')\n",
    "        else:\n",
    "            new_text.append(word)\n",
    "            \n",
    "    x=new_text[:]\n",
    "    new_text.clear()\n",
    "    return \" \".join(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(df):\n",
    "    df['text'] = df['text'].apply(lambda x: contractions.fix(x))\n",
    "    df['text'] = df['text'].apply(lambda x: demojize_text(x))\n",
    "    df['text'] = df['text'].apply(lambda x: clean_text(x))\n",
    "    df['text'] = df['text'].apply(lambda x: remove_stopwords(x))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9m/lzs50f6d6wvbxft4kvrymlf80000gn/T/ipykernel_18553/1529191381.py:11: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  text = BeautifulSoup(text, 'html.parser').get_text()\n"
     ]
    }
   ],
   "source": [
    "train_df = text_preprocessing(train_df)\n",
    "test_df = text_preprocessing(test_df)\n",
    "valid_df = text_preprocessing(valid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import BertTokenizer, BertModel\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "   device = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 256\n",
    "TRAIN_BATCH_SIZE = 16\n",
    "VALID_BATCH_SIZE = 8\n",
    "EPOCHS = 5\n",
    "LEARNING_RATE = 2e-05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionDataset:\n",
    "    def __init__(self,df,labels,tokenizer,max_len):\n",
    "        self.data = df.text\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.targets = df[labels].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        text = self.data[idx]\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.targets[idx], dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Weights: admiration         0.375389\n",
      "amusement          0.665961\n",
      "anger              0.989379\n",
      "annoyance          0.627675\n",
      "approval           0.527512\n",
      "caring             1.426272\n",
      "confusion          1.133302\n",
      "curiosity          0.707603\n",
      "desire             2.418654\n",
      "disappointment     1.221716\n",
      "disapproval        0.766744\n",
      "disgust            1.955053\n",
      "embarrassment      5.116690\n",
      "excitement         1.817535\n",
      "fear               2.601270\n",
      "gratitude          0.582403\n",
      "grief             20.134508\n",
      "joy                1.067739\n",
      "love               0.743220\n",
      "nervousness        9.453397\n",
      "optimism           0.980618\n",
      "pride             13.967181\n",
      "realization        1.396718\n",
      "relief            10.133053\n",
      "remorse            2.844692\n",
      "sadness            1.169198\n",
      "surprise           1.462601\n",
      "neutral            0.109034\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Flatten the binary labels to calculate frequency per class\n",
    "class_counts = train_df[emotions].sum(axis=0)\n",
    "total_samples = train_df[emotions].shape[0]\n",
    "\n",
    "# Compute weights: inverse frequency\n",
    "class_weights = total_samples / (len(emotions) * class_counts)\n",
    "\n",
    "print(\"Class Weights:\", class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = EmotionDataset(train_df,emotions, tokenizer, MAX_LEN)\n",
    "validation_set = EmotionDataset(valid_df,emotions, tokenizer, MAX_LEN)\n",
    "testing_set = EmotionDataset(test_df,emotions, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(training_set, batch_size=TRAIN_BATCH_SIZE, \n",
    "                          num_workers=0, shuffle=True, pin_memory=True)\n",
    "valid_loader = DataLoader(validation_set, batch_size=VALID_BATCH_SIZE, \n",
    "                          num_workers=0, shuffle=False, pin_memory=True)\n",
    "test_loader = DataLoader(testing_set, batch_size=VALID_BATCH_SIZE, \n",
    "                          num_workers=0, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RoBERTaClass(\n",
       "  (l1): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): RobertaPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (l2): Dropout(p=0.3, inplace=False)\n",
       "  (l3): Linear(in_features=768, out_features=28, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class RoBERTaClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RoBERTaClass, self).__init__()\n",
    "        self.l1 = RobertaModel.from_pretrained('roberta-base', return_dict=True)\n",
    "        self.l2 = torch.nn.Dropout(0.3)\n",
    "        self.l3 = torch.nn.Linear(768, 28)\n",
    "\n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        _, output_1= self.l1(ids, attention_mask = mask, token_type_ids = token_type_ids, return_dict=False)\n",
    "        output_2 = self.l2(output_1)\n",
    "        output = self.l3(output_2)\n",
    "        return output\n",
    "\n",
    "model = RoBERTaClass()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(outputs,class_weights, targets):\n",
    "    class_weights_tensor = torch.tensor(class_weights, dtype=torch.float)\n",
    "    class_weights_tensor = class_weights_tensor.to(device)\n",
    "    return torch.nn.BCEWithLogitsLoss(pos_weight=class_weights_tensor)(outputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sudeepmungara/Documents/Personal_Projects/NLP/venv/lib/python3.12/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "total_steps = len(train_loader) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(epoch, class_weights):\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0  # Initialize running loss\n",
    "    total_batches = len(train_loader)  # Total number of batches\n",
    "    accumulation_steps = 4  # Accumulate gradients to simulate larger batch size\n",
    "\n",
    "    for batch_idx, data in enumerate(train_loader, 1):  # Start counting batches from 1\n",
    "        # Move data to the device\n",
    "        input_ids = data['ids'].to(device, dtype=torch.long)\n",
    "        attention_mask = data['mask'].to(device, dtype=torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n",
    "        targets = data['targets'].to(device, dtype=torch.float)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids, attention_mask, token_type_ids)\n",
    "        # Compute loss\n",
    "        loss = loss_fn(outputs, class_weights, targets)\n",
    "        loss = loss / accumulation_steps  # Scale loss for accumulation\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Perform optimization step only after accumulation steps\n",
    "        if (batch_idx % accumulation_steps) == 0:\n",
    "            optimizer.step()\n",
    "            scheduler.step()  # Update learning rate\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # Accumulate loss\n",
    "        running_loss += loss.item() * accumulation_steps  # Undo the scaling for tracking\n",
    "\n",
    "    # Compute average loss for the epoch\n",
    "    average_loss = running_loss / total_batches\n",
    "    print(f'\\nEpoch: {epoch} completed. Average Loss: {average_loss:.4f}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation():\n",
    "    model.eval()\n",
    "    fin_targets=[]\n",
    "    fin_outputs=[]\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(test_loader, 0):\n",
    "            ids = data['ids'].to(device, dtype = torch.long)\n",
    "            mask = data['mask'].to(device, dtype = torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "            targets = data['targets'].to(device, dtype = torch.float)\n",
    "            outputs = model(ids, mask, token_type_ids)\n",
    "            fin_targets.extend(targets.cpu().detach().numpy().tolist())\n",
    "            fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
    "    return fin_outputs, fin_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9m/lzs50f6d6wvbxft4kvrymlf80000gn/T/ipykernel_18553/1314199939.py:2: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  class_weights_tensor = torch.tensor(class_weights, dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0 completed. Average Loss: 0.1758\n",
      "\n",
      "F1 Score (Micro) -t1 = 0.00785052598524101\n",
      "F1 Score (Macro) -t1 = 0.018601190476190476\n",
      "F1 Score (Micro) -t2 = 0.02311960542540074\n",
      "F1 Score (Macro) -t2 = 0.04367287540774253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9m/lzs50f6d6wvbxft4kvrymlf80000gn/T/ipykernel_18553/1314199939.py:2: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  class_weights_tensor = torch.tensor(class_weights, dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1 completed. Average Loss: 0.1207\n",
      "\n",
      "F1 Score (Micro) -t1 = 0.2315873437915448\n",
      "F1 Score (Macro) -t1 = 0.24081845555017917\n",
      "F1 Score (Micro) -t2 = 0.31791975526532534\n",
      "F1 Score (Macro) -t2 = 0.32285622973955486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9m/lzs50f6d6wvbxft4kvrymlf80000gn/T/ipykernel_18553/1314199939.py:2: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  class_weights_tensor = torch.tensor(class_weights, dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 2 completed. Average Loss: 0.1012\n",
      "\n",
      "F1 Score (Micro) -t1 = 0.3246250604741171\n",
      "F1 Score (Macro) -t1 = 0.3312325012681277\n",
      "F1 Score (Micro) -t2 = 0.37608131487889274\n",
      "F1 Score (Macro) -t2 = 0.37243289857823964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9m/lzs50f6d6wvbxft4kvrymlf80000gn/T/ipykernel_18553/1314199939.py:2: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  class_weights_tensor = torch.tensor(class_weights, dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 3 completed. Average Loss: 0.0919\n",
      "\n",
      "F1 Score (Micro) -t1 = 0.3645904631530915\n",
      "F1 Score (Macro) -t1 = 0.3645036822713374\n",
      "F1 Score (Micro) -t2 = 0.40468858647064665\n",
      "F1 Score (Macro) -t2 = 0.3902262615500729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9m/lzs50f6d6wvbxft4kvrymlf80000gn/T/ipykernel_18553/1314199939.py:2: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  class_weights_tensor = torch.tensor(class_weights, dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 4 completed. Average Loss: 0.0855\n",
      "\n",
      "F1 Score (Micro) -t1 = 0.37701657458563537\n",
      "F1 Score (Macro) -t1 = 0.37237325017439143\n",
      "F1 Score (Micro) -t2 = 0.40340076223981236\n",
      "F1 Score (Macro) -t2 = 0.3857807002784775\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    train(epoch,class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs, targets = validation()\n",
    "\n",
    "print(\"Scores with 0.5 threshold\")\n",
    "outputs_t1 = np.array(outputs) >= 0.5\n",
    "\n",
    "f1_score_micro = metrics.f1_score(targets, outputs_t1, average='micro')\n",
    "f1_score_macro = metrics.f1_score(targets, outputs_t1, average='macro')\n",
    "\n",
    "print(f\"F1 Score (Micro) = {f1_score_micro}\")\n",
    "print(f\"F1 Score (Macro) = {f1_score_macro}\")\n",
    "\n",
    "print(\"Scores with 0.3 threshold\")\n",
    "outputs_t2 = np.array(outputs) >= 0.3\n",
    "\n",
    "f1_score_micro = metrics.f1_score(targets, outputs_t2, average='micro')\n",
    "f1_score_macro = metrics.f1_score(targets, outputs_t2, average='macro')\n",
    "\n",
    "print(f\"F1 Score (Micro) = {f1_score_micro}\")\n",
    "print(f\"F1 Score (Macro) = {f1_score_macro}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
